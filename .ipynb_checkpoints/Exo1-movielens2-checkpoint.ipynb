{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Implementing Matrix Factorization from Scratch\n",
    "\n",
    "**Course:** Recommender Systems <br>\n",
    "**Professor:** Guilherme MEDEIROS MACHADO <br>\n",
    "**Topic:** Collaborative Filtering with Matrix Factorization\n",
    "\n",
    "---\n",
    "\n",
    "## Goal of the Exercise\n",
    "\n",
    "The objective of this exercise is to build a movie recommender system by implementing the **Matrix Factorization** algorithm from scratch using Python. We will use the famous **MovieLens 100k** dataset. By the end of this notebook, you will have:\n",
    "\n",
    "1.  Understood the theoretical foundations of matrix factorization.\n",
    "2.  Implemented the algorithm using **Stochastic Gradient Descent (SGD)**.\n",
    "3.  Trained your model on real-world movie rating data.\n",
    "4.  Evaluated your model's performance using Root Mean Squared Error (RMSE).\n",
    "5.  Generated personalized top-10 movie recommendations for a specific user.\n",
    "\n",
    "This exercise forbids the use of pre-built matrix factorization libraries (like `surprise`, `lightfm`, etc.) to ensure you gain a deep understanding of the inner workings of the algorithm.\n",
    "\n",
    "---\n",
    "\n",
    "## The Dataset: MovieLens 100k\n",
    "\n",
    "We will be using the MovieLens 100k dataset, a classic dataset in the recommender systems community. It contains:\n",
    "* 100,000 ratings (1-5) from...\n",
    "* 943 users on...\n",
    "* 1682 movies.\n",
    "\n",
    "You will need two files from this dataset:\n",
    "* `u.data`: The full dataset of 100k ratings. Each row is in the format: `user_id`, `item_id`, `rating`, `timestamp`.\n",
    "* `u.item`: Information about the movies (items). Each row contains the `item_id`, `movie_title`, and other metadata. We'll use it to get the movie names for our final recommendations.\n",
    "\n",
    "Let's start by downloading and exploring the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading MovieLens 100k dataset...\n",
      "Download and extraction complete.\n",
      "Data loaded successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>881250949</td>\n",
       "      <td>Kolya (1996)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "      <td>L.A. Confidential (1997)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>878887116</td>\n",
       "      <td>Heavyweights (1994)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "      <td>Legends of the Fall (1994)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>886397596</td>\n",
       "      <td>Jackie Brown (1997)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating  timestamp                       title\n",
       "0      196      242       3  881250949                Kolya (1996)\n",
       "1      186      302       3  891717742    L.A. Confidential (1997)\n",
       "2       22      377       1  878887116         Heavyweights (1994)\n",
       "3      244       51       2  880606923  Legends of the Fall (1994)\n",
       "4      166      346       1  886397596         Jackie Brown (1997)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "\n",
    "# --- Download the dataset if it doesn't exist ---\n",
    "if not os.path.exists('ml-100k'):\n",
    "    print(\"Downloading MovieLens 100k dataset...\")\n",
    "    url = 'http://files.grouplens.org/datasets/movielens/ml-100k.zip'\n",
    "    urlretrieve(url, 'ml-100k.zip')\n",
    "    with zipfile.ZipFile('ml-100k.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall()\n",
    "    print(\"Download and extraction complete.\")\n",
    "\n",
    "# --- Load the data ---\n",
    "# u.data contains the ratings\n",
    "data_cols = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "ratings_df = pd.read_csv('ml-100k/u.data', sep='\\t', names=data_cols)\n",
    "\n",
    "# u.item contains movie titles\n",
    "item_cols = ['item_id', 'title'] + [f'col{i}' for i in range(22)] # Remaining columns are not needed\n",
    "movies_df = pd.read_csv('ml-100k/u.item', sep='|', names=item_cols, encoding='latin-1', usecols=['item_id', 'title'])\n",
    "\n",
    "# Merge the two dataframes to have movie titles and ratings in one place\n",
    "df = pd.merge(ratings_df, movies_df, on='item_id')\n",
    "\n",
    "print(\"Data loaded successfully!\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Data Preparation\n",
    "\n",
    "The raw data is a list of ratings. For matrix factorization, it's conceptually easier to think of our data as a large **user-item interaction matrix**, let's call it $R$. In this matrix:\n",
    "* The rows represent users.\n",
    "* The columns represent movies (items).\n",
    "* The value at cell $(u, i)$, denoted $R_{ui}$, is the rating user $u$ gave to movie $i$.\n",
    "\n",
    "This matrix is typically very **sparse**, as most users have only rated a small fraction of the available movies.\n",
    "\n",
    "Let's create this matrix using a Pandas pivot table. This will also help us determine the number of unique users and movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_user_item_matrix(df):\n",
    "    \"\"\"\n",
    "    Creates the user-item interaction matrix from the dataframe.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe containing user_id, item_id, and rating.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A user-item matrix with users as rows, items as columns, and ratings as values.\n",
    "                       NaNs indicate that a user has not rated an item.\n",
    "    \"\"\"\n",
    "    user_item_matrix = df.pivot_table(\n",
    "        index=\"userId\",   # lignes = utilisateurs\n",
    "        columns=\"movieId\", # colonnes = items\n",
    "        values=\"rating\",  # valeurs = notes\n",
    "        aggfunc=\"mean\"    # si doublons → moyenne\n",
    "    )\n",
    "    return user_item_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: The Theory of Matrix Factorization\n",
    "\n",
    "The core idea is to **decompose** our large, sparse user-item matrix $R$ (size $m \\times n$) into two smaller, dense matrices:\n",
    "1.  A **user-feature matrix** $P$ (size $m \\times k$).\n",
    "2.  An **item-feature matrix** $Q$ (size $n \\times k$).\n",
    "\n",
    "Here, $k$ is the number of **latent factors**, which is a hyperparameter we choose. These latent factors represent hidden characteristics of users and items. For movies, a factor might represent the \"amount of comedy\" vs. \"drama\", or \"blockbuster\" vs. \"indie film\". For users, a factor might represent their preference for these characteristics.\n",
    "\n",
    "\n",
    "\n",
    "The prediction of a rating $\\hat{r}_{ui}$ that user $u$ would give to item $i$ is calculated by the dot product of the user's latent vector $p_u$ and the item's latent vector $q_i$:\n",
    "\n",
    "$$\\hat{r}_{ui} = p_u \\cdot q_i^T = \\sum_{k=1}^{K} p_{uk} q_{ik}$$\n",
    "\n",
    "Our goal is to find the matrices $P$ and $Q$ such that their product $P \\cdot Q^T$ is as close as possible to the known ratings in our original matrix $R$. We formalize this using a **loss function**. A common choice is the sum of squared errors, with **regularization** to prevent overfitting:\n",
    "\n",
    "$$L = \\sum_{(u,i) \\in \\mathcal{K}} (r_{ui} - \\hat{r}_{ui})^2 + \\lambda \\left( \\sum_{u} ||p_u||^2 + \\sum_{i} ||q_i||^2 \\right)$$\n",
    "\n",
    "Where:\n",
    "* $\\mathcal{K}$ is the set of $(u, i)$ pairs for which the rating $r_{ui}$ is known.\n",
    "* $\\lambda$ is the regularization parameter, another hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: The Algorithm - Stochastic Gradient Descent (SGD)\n",
    "\n",
    "To minimize our loss function $L$, we will use **Stochastic Gradient Descent (SGD)**. Instead of calculating the gradient over all known ratings (which is computationally expensive), SGD iterates through each known rating one by one and updates the parameters in the direction that minimizes the error for that single rating.\n",
    "\n",
    "For each known rating $r_{ui}$:\n",
    "1.  Calculate the prediction error: $e_{ui} = r_{ui} - \\hat{r}_{ui}$\n",
    "2.  Update the user and item latent vectors ($p_u$ and $q_i$) using the following update rules:\n",
    "\n",
    "$$p_u \\leftarrow p_u + \\alpha \\cdot (e_{ui} \\cdot q_i - \\lambda \\cdot p_u)$$\n",
    "$$q_i \\leftarrow q_i + \\alpha \\cdot (e_{ui} \\cdot p_u - \\lambda \\cdot q_i)$$\n",
    "\n",
    "Where:\n",
    "* $\\alpha$ is the **learning rate**, a hyperparameter that controls the step size.\n",
    "\n",
    "We repeat this process for a fixed number of **epochs** (iterations over the entire training dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Step-by-Step Implementation\n",
    "\n",
    "Let's build our model. First, we need to split our data into a training and a testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def matrix_factorization_SGD(df, n_users, n_items, k=20, \n",
    "                             n_epochs=50, alpha=0.01, lam=0.1, seed=42):\n",
    "    \"\"\"\n",
    "    Matrix Factorization with SGD.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): must contain columns ['u','i','rating'] \n",
    "                           (users/items déjà encodés en indices 0..n-1).\n",
    "        n_users (int): nombre d’utilisateurs.\n",
    "        n_items (int): nombre d’items.\n",
    "        k (int): nombre de facteurs latents.\n",
    "        n_epochs (int): nombre de passes sur le dataset.\n",
    "        alpha (float): learning rate.\n",
    "        lam (float): régularisation L2.\n",
    "        seed (int): graine RNG.\n",
    "\n",
    "    Returns:\n",
    "        P (np.array): matrice user-feature (n_users × k).\n",
    "        Q (np.array): matrice item-feature (n_items × k).\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    P = 0.1 * rng.standard_normal((n_users, k))\n",
    "    Q = 0.1 * rng.standard_normal((n_items, k))\n",
    "\n",
    "    samples = df[['u','i','rating']].to_numpy()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        rng.shuffle(samples)\n",
    "        for u, i, r in samples:\n",
    "            u, i = int(u), int(i)\n",
    "            # prédiction\n",
    "            r_hat = P[u] @ Q[i].T\n",
    "            # erreur\n",
    "            e_ui = r - r_hat\n",
    "            # updates\n",
    "            P[u] += alpha * (e_ui * Q[i] - lam * P[u])\n",
    "            Q[i] += alpha * (e_ui * P[u] - lam * Q[i])\n",
    "\n",
    "    return P, Q\n",
    "\n",
    "def predict_rating(P, Q, u, i):\n",
    "    \"\"\"Prédit la note de l’utilisateur u pour l’item i.\"\"\"\n",
    "    return P[u] @ Q[i].T\n",
    "\n",
    "def recommend_MF(P, Q, u_idx, train_df, idx2item, movies, topk=5):\n",
    "    \"\"\"Reco top-k pour un user donné (exclut les items déjà notés).\"\"\"\n",
    "    scores = P[u_idx] @ Q.T\n",
    "    already = train_df[train_df['u']==u_idx]['i'].values\n",
    "    scores[already] = -np.inf\n",
    "    top_items = np.argsort(-scores)[:topk]\n",
    "    recs = []\n",
    "    for j in top_items:\n",
    "        mid = int(idx2item[j])\n",
    "        title = movies.loc[movies['movieId']==mid,'title']\n",
    "        title = str(title.values[0]) if len(title)>0 else f\"item {mid}\"\n",
    "        recs.append((mid, title, float(scores[j])))\n",
    "    return recs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Initialization\n",
    "\n",
    "We need to initialize our user-feature matrix $P$ and item-feature matrix $Q$ with small random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_matrices(n_users, n_items, n_factors, seed=42):\n",
    "    \"\"\"\n",
    "    Initializes the user-feature (P) and item-feature (Q) matrices.\n",
    "\n",
    "    Args:\n",
    "        n_users (int): Number of users.\n",
    "        n_items (int): Number of items.\n",
    "        n_factors (int): Number of latent factors.\n",
    "        seed (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - P (np.ndarray): The user-feature matrix (n_users x n_factors).\n",
    "            - Q (np.ndarray): The item-feature matrix (n_items x n_factors).\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    # small random values from standard normal, scaled down\n",
    "    P = 0.1 * rng.standard_normal((n_users, n_factors))\n",
    "    Q = 0.1 * rng.standard_normal((n_items, n_factors))\n",
    "    return P, Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 The Training Loop (SGD)\n",
    "\n",
    "This is the core of our algorithm. We will loop for a specified number of epochs. In each epoch, we will iterate over all known ratings in our training set `R_train` and update the corresponding user and item vectors in `P` and `Q`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(R_train, P, Q, learning_rate, regularization, epochs, seed=42):\n",
    "    \"\"\"\n",
    "    Trains the matrix factorization model using SGD.\n",
    "\n",
    "    Args:\n",
    "        R_train (np.ndarray): The training user-item matrix. Missing entries should be NaN.\n",
    "        P (np.ndarray): The user-feature matrix (n_users x n_factors).\n",
    "        Q (np.ndarray): The item-feature matrix (n_items x n_factors).\n",
    "        learning_rate (float): The learning rate (alpha).\n",
    "        regularization (float): The regularization parameter (lambda).\n",
    "        epochs (int): The number of iterations over the training data.\n",
    "        seed (int): RNG seed for shuffling.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (P, Q) trained matrices.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # indices (u, i) où on a une note connue (non-NaN)\n",
    "    if np.issubdtype(R_train.dtype, np.floating):\n",
    "        known_mask = ~np.isnan(R_train)\n",
    "    else:\n",
    "        # fallback si pas de NaN: on considère les non-zéros comme observés\n",
    "        known_mask = R_train != 0\n",
    "\n",
    "    user_idx, item_idx = np.where(known_mask)\n",
    "    samples = np.stack([user_idx, item_idx], axis=1)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        rng.shuffle(samples)\n",
    "        for u, i in samples:\n",
    "            r_ui = R_train[u, i]\n",
    "            # prédiction\n",
    "            r_hat = P[u] @ Q[i]\n",
    "            # erreur\n",
    "            e_ui = r_ui - r_hat\n",
    "            # mises à jour (SGD) avec régularisation L2\n",
    "            Pu = P[u].copy()  # éviter l'update en cascade dans Q\n",
    "            P[u] += learning_rate * (e_ui * Q[i] - regularization * P[u])\n",
    "            Q[i] += learning_rate * (e_ui * Pu   - regularization * Q[i])\n",
    "\n",
    "    return P, Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Evaluation\n",
    "\n",
    "After training, we must evaluate how well our model performs on unseen data. We will use the **Root Mean Squared Error (RMSE)**, which measures the average magnitude of the errors between predicted and actual ratings.\n",
    "\n",
    "The formula is:\n",
    "$$RMSE = \\sqrt{\\frac{1}{|\\mathcal{T}|} \\sum_{(u,i) \\in \\mathcal{T}} (r_{ui} - \\hat{r}_{ui})^2}$$\n",
    "\n",
    "Where $\\mathcal{T}$ is the set of ratings in our test set. A lower RMSE means better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rmse(R_test, P, Q):\n",
    "    \"\"\"\n",
    "    Calculates the Root Mean Squared Error (RMSE) on the test set.\n",
    "\n",
    "    Args:\n",
    "        R_test (np.ndarray): The testing user-item matrix (NaN for missing ratings).\n",
    "        P (np.ndarray): The trained user-feature matrix.\n",
    "        Q (np.ndarray): The trained item-feature matrix.\n",
    "\n",
    "    Returns:\n",
    "        float: The RMSE value.\n",
    "    \"\"\"\n",
    "    if np.issubdtype(R_test.dtype, np.floating):\n",
    "        mask = ~np.isnan(R_test)   # uniquement les ratings connus\n",
    "    else:\n",
    "        mask = R_test != 0\n",
    "\n",
    "    user_idx, item_idx = np.where(mask)\n",
    "    errors = []\n",
    "    for u, i in zip(user_idx, item_idx):\n",
    "        r_true = R_test[u, i]\n",
    "        r_pred = P[u] @ Q[i]\n",
    "        errors.append((r_true - r_pred) ** 2)\n",
    "\n",
    "    mse = np.mean(errors) if errors else 0.0\n",
    "    return np.sqrt(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Putting It All Together\n",
    "\n",
    "Now, let's connect all the pieces. We'll set our hyperparameters, initialize our matrices, train the model, and finally evaluate it.\n",
    "\n",
    "**Your Goal:** Tune the hyperparameters to achieve an **RMSE below 0.98**. A good model can even reach ~0.95. If your RMSE is higher, try adjusting the learning rate, regularization, number of factors, or epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]]\n",
      "[[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]]\n",
      "Users=943, Items=1682, Train obs=80000, Test obs=20000\n",
      "Test RMSE = 0.9815  (objectif < 0.98, viser ~0.95 avec un bon tuning)\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Part 6 — Putting It All Together (fix: user_id/item_id/rating)\n",
    "# ===============================\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# --- (A) User–item matrix depuis df (user_id, item_id, rating) ---------------\n",
    "R_df = df.pivot_table(\n",
    "    index=\"user_id\",\n",
    "    columns=\"item_id\",\n",
    "    values=\"rating\",\n",
    "    aggfunc=\"mean\"\n",
    ")\n",
    "R_np = R_df.to_numpy().astype(float)\n",
    "n_users, n_items = R_np.shape\n",
    "\n",
    "# --- (B) Train/Test split sur les entrées observées --------------------------\n",
    "rng = np.random.default_rng(42)\n",
    "known_mask = ~np.isnan(R_np)\n",
    "u_idx, i_idx = np.where(known_mask)\n",
    "perm = rng.permutation(len(u_idx))\n",
    "\n",
    "test_size = int(0.20 * len(u_idx))\n",
    "test_sel = perm[:test_size]\n",
    "train_sel = perm[test_size:]\n",
    "\n",
    "R_train = np.full_like(R_np, np.nan)\n",
    "R_test  = np.full_like(R_np, np.nan)\n",
    "\n",
    "R_train[u_idx[train_sel], i_idx[train_sel]] = R_np[u_idx[train_sel], i_idx[train_sel]]\n",
    "R_test[u_idx[test_sel],   i_idx[test_sel]]   = R_np[u_idx[test_sel],   i_idx[test_sel]]\n",
    "\n",
    "print(f\"Users={n_users}, Items={n_items}, \"\n",
    "      f\"Train obs={np.sum(~np.isnan(R_train))}, Test obs={np.sum(~np.isnan(R_test))}\")\n",
    "\n",
    "# --- (C) Hyperparams ---------------------------------------------------------\n",
    "k      = 32      # latent factors\n",
    "alpha  = 0.02    # learning rate\n",
    "lam    = 0.05    # regularization\n",
    "epochs = 50      # epochs\n",
    "\n",
    "# --- (D) Init ---------------------------------------------------------------\n",
    "P, Q = initialize_matrices(n_users, n_items, k, seed=42)\n",
    "\n",
    "# --- (E) Training (SGD) -----------------------------------------------------\n",
    "P, Q = train_model(R_train, P, Q,\n",
    "                   learning_rate=alpha,\n",
    "                   regularization=lam,\n",
    "                   epochs=epochs,\n",
    "                   seed=42)\n",
    "\n",
    "# --- (F) Evaluation (RMSE) --------------------------------------------------\n",
    "rmse = calculate_rmse(R_test, P, Q)\n",
    "print(f\"Test RMSE = {rmse:.4f}  (objectif < 0.98, viser ~0.95 avec un bon tuning)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Making Recommendations\n",
    "\n",
    "The ultimate goal is to recommend movies! Now that we have our trained matrices $P$ and $Q$, we can predict the rating for *any* user-item pair, including those the user has not seen yet.\n",
    "\n",
    "The process for a given user `user_id`:\n",
    "1.  Get the user's latent vector $p_u$ from the trained matrix $P$.\n",
    "2.  Calculate the predicted ratings for all items by taking the dot product of $p_u$ and the entire item-feature matrix $Q^T$.\n",
    "3.  Create a list of movie titles and their predicted ratings.\n",
    "4.  Filter out movies the user has already seen.\n",
    "5.  Sort the remaining movies by their predicted rating in descending order.\n",
    "6.  Return the top N movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_top_movies(user_id, P, Q, movie_titles_df, R_df, top_n=10):\n",
    "    \"\"\"\n",
    "    Recommends top N movies for a given user.\n",
    "\n",
    "    Args:\n",
    "        user_id (int): The ID of the user (as in R_df.index).\n",
    "        P (np.ndarray): Trained user-feature matrix, shape (n_users, k).\n",
    "        Q (np.ndarray): Trained item-feature matrix, shape (n_items, k).\n",
    "        movie_titles_df (pd.DataFrame): Must contain columns ['item_id','title'].\n",
    "        R_df (pd.DataFrame): User–item matrix (index=user_id, columns=item_id). NaN = unseen.\n",
    "        top_n (int): Number of movies to recommend.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Columns ['item_id','title','pred_rating'] sorted by pred_rating desc.\n",
    "    \"\"\"\n",
    "    # 1) récupérer l’index numpy du user (0-based) depuis l’index du pivot\n",
    "    if user_id not in R_df.index:\n",
    "        raise ValueError(f\"user_id {user_id} not found in R_df.index\")\n",
    "    u_idx = R_df.index.get_loc(user_id)\n",
    "\n",
    "    # 2) prédictions pour tous les items\n",
    "    scores = P[u_idx] @ Q.T  # shape (n_items,)\n",
    "\n",
    "    # 3) filtrer les items déjà vus par l’utilisateur\n",
    "    seen_mask = ~R_df.loc[user_id].isna()\n",
    "    # col order of R_df corresponds to item indices used for Q rows\n",
    "    item_ids = R_df.columns.to_numpy()              # shape (n_items,)\n",
    "    scores = scores.astype(float)\n",
    "\n",
    "    # mettre -inf sur les items déjà vus\n",
    "    scores[seen_mask.values] = -np.inf\n",
    "\n",
    "    # 4) top-N indices\n",
    "    top_idx = np.argsort(-scores)[:top_n]\n",
    "    top_item_ids = item_ids[top_idx]\n",
    "    top_scores = scores[top_idx]\n",
    "\n",
    "    # 5) joindre les titres\n",
    "    out = pd.DataFrame({\n",
    "        \"item_id\": top_item_ids,\n",
    "        \"pred_rating\": top_scores\n",
    "    })\n",
    "    out = out.merge(movie_titles_df[[\"item_id\",\"title\"]], on=\"item_id\", how=\"left\")\n",
    "\n",
    "    # 6) trier et renvoyer\n",
    "    out = out.sort_values(\"pred_rating\", ascending=False).reset_index(drop=True)\n",
    "    return out[[\"item_id\",\"title\",\"pred_rating\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>title</th>\n",
       "      <th>pred_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>522</td>\n",
       "      <td>Down by Law (1986)</td>\n",
       "      <td>5.855224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>647</td>\n",
       "      <td>Ran (1985)</td>\n",
       "      <td>5.760410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1449</td>\n",
       "      <td>Pather Panchali (1955)</td>\n",
       "      <td>5.463517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>285</td>\n",
       "      <td>Secrets &amp; Lies (1996)</td>\n",
       "      <td>5.384231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>483</td>\n",
       "      <td>Casablanca (1942)</td>\n",
       "      <td>5.252466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1405</td>\n",
       "      <td>Boy's Life 2 (1997)</td>\n",
       "      <td>5.237434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>793</td>\n",
       "      <td>Crooklyn (1994)</td>\n",
       "      <td>5.187207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>922</td>\n",
       "      <td>Dead Man (1995)</td>\n",
       "      <td>5.170497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>638</td>\n",
       "      <td>Return of Martin Guerre, The (Retour de Martin...</td>\n",
       "      <td>5.154755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>902</td>\n",
       "      <td>Big Lebowski, The (1998)</td>\n",
       "      <td>5.126638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_id                                              title  pred_rating\n",
       "0      522                                 Down by Law (1986)     5.855224\n",
       "1      647                                         Ran (1985)     5.760410\n",
       "2     1449                             Pather Panchali (1955)     5.463517\n",
       "3      285                              Secrets & Lies (1996)     5.384231\n",
       "4      483                                  Casablanca (1942)     5.252466\n",
       "5     1405                                Boy's Life 2 (1997)     5.237434\n",
       "6      793                                    Crooklyn (1994)     5.187207\n",
       "7      922                                    Dead Man (1995)     5.170497\n",
       "8      638  Return of Martin Guerre, The (Retour de Martin...     5.154755\n",
       "9      902                           Big Lebowski, The (1998)     5.126638"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "recs = recommend_top_movies(user_id=1, P=P, Q=Q, movie_titles_df=movies_df, R_df=R_df, top_n=10)\n",
    "display(recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
