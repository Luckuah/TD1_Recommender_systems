{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Implementing Matrix Factorization from Scratch\n",
    "\n",
    "**Course:** Recommender Systems <br>\n",
    "**Professor:** Guilherme MEDEIROS MACHADO <br>\n",
    "**Topic:** Collaborative Filtering with Matrix Factorization\n",
    "\n",
    "---\n",
    "\n",
    "## Goal of the Exercise\n",
    "\n",
    "The objective of this exercise is to build a movie recommender system by implementing the **Matrix Factorization** algorithm from scratch using Python. We will use the famous **MovieLens 100k** dataset. By the end of this notebook, you will have:\n",
    "\n",
    "1.  Understood the theoretical foundations of matrix factorization.\n",
    "2.  Implemented the algorithm using **Stochastic Gradient Descent (SGD)**.\n",
    "3.  Trained your model on real-world movie rating data.\n",
    "4.  Evaluated your model's performance using Root Mean Squared Error (RMSE).\n",
    "5.  Generated personalized top-10 movie recommendations for a specific user.\n",
    "\n",
    "This exercise forbids the use of pre-built matrix factorization libraries (like `surprise`, `lightfm`, etc.) to ensure you gain a deep understanding of the inner workings of the algorithm.\n",
    "\n",
    "---\n",
    "\n",
    "## The Dataset: MovieLens 100k\n",
    "\n",
    "We will be using the MovieLens 100k dataset, a classic dataset in the recommender systems community. It contains:\n",
    "* 100,000 ratings (1-5) from...\n",
    "* 943 users on...\n",
    "* 1682 movies.\n",
    "\n",
    "You will need two files from this dataset:\n",
    "* `u.data`: The full dataset of 100k ratings. Each row is in the format: `user_id`, `item_id`, `rating`, `timestamp`.\n",
    "* `u.item`: Information about the movies (items). Each row contains the `item_id`, `movie_title`, and other metadata. We'll use it to get the movie names for our final recommendations.\n",
    "\n",
    "Let's start by downloading and exploring the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>881250949</td>\n",
       "      <td>Kolya (1996)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "      <td>L.A. Confidential (1997)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>878887116</td>\n",
       "      <td>Heavyweights (1994)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "      <td>Legends of the Fall (1994)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>886397596</td>\n",
       "      <td>Jackie Brown (1997)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating  timestamp                       title\n",
       "0      196      242       3  881250949                Kolya (1996)\n",
       "1      186      302       3  891717742    L.A. Confidential (1997)\n",
       "2       22      377       1  878887116         Heavyweights (1994)\n",
       "3      244       51       2  880606923  Legends of the Fall (1994)\n",
       "4      166      346       1  886397596         Jackie Brown (1997)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "\n",
    "# --- Download the dataset if it doesn't exist ---\n",
    "if not os.path.exists('ml-100k'):\n",
    "    print(\"Downloading MovieLens 100k dataset...\")\n",
    "    url = 'http://files.grouplens.org/datasets/movielens/ml-100k.zip'\n",
    "    urlretrieve(url, 'ml-100k.zip')\n",
    "    with zipfile.ZipFile('ml-100k.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall()\n",
    "    print(\"Download and extraction complete.\")\n",
    "\n",
    "# --- Load the data ---\n",
    "# u.data contains the ratings\n",
    "data_cols = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "ratings_df = pd.read_csv('ml-100k/u.data', sep='\\t', names=data_cols)\n",
    "\n",
    "# u.item contains movie titles\n",
    "item_cols = ['item_id', 'title'] + [f'col{i}' for i in range(22)] # Remaining columns are not needed\n",
    "movies_df = pd.read_csv('ml-100k/u.item', sep='|', names=item_cols, encoding='latin-1', usecols=['item_id', 'title'])\n",
    "\n",
    "# Merge the two dataframes to have movie titles and ratings in one place\n",
    "df = pd.merge(ratings_df, movies_df, on='item_id')\n",
    "\n",
    "print(\"Data loaded successfully!\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Data Preparation\n",
    "\n",
    "The raw data is a list of ratings. For matrix factorization, it's conceptually easier to think of our data as a large **user-item interaction matrix**, let's call it $R$. In this matrix:\n",
    "* The rows represent users.\n",
    "* The columns represent movies (items).\n",
    "* The value at cell $(u, i)$, denoted $R_{ui}$, is the rating user $u$ gave to movie $i$.\n",
    "\n",
    "This matrix is typically very **sparse**, as most users have only rated a small fraction of the available movies.\n",
    "\n",
    "Let's create this matrix using a Pandas pivot table. This will also help us determine the number of unique users and movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:Your code here\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "    \n",
    "def create_user_item_matrix(df):\n",
    "    \"\"\"\n",
    "    Creates the user-item interaction matrix from the dataframe.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe containing user_id, item_id, and rating.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A user-item matrix with users as rows, items as columns, and ratings as values.\n",
    "                       NaNs indicate that a user has not rated an item.\n",
    "    \"\"\"\n",
    "\n",
    "    R_df = df.pivot_table(\n",
    "    index=\"user_id\",\n",
    "    columns=\"item_id\",\n",
    "    values=\"rating\",\n",
    "    aggfunc=\"mean\")\n",
    "    \n",
    "    R_np = R_df.to_numpy().astype(float)\n",
    "    \n",
    "    rng = np.random.default_rng(42)\n",
    "    known_mask = ~np.isnan(R_np)\n",
    "    u_idx, i_idx = np.where(known_mask)\n",
    "    perm = rng.permutation(len(u_idx))\n",
    "    \n",
    "    test_size = int(0.20 * len(u_idx))\n",
    "    test_sel = perm[:test_size]\n",
    "    train_sel = perm[test_size:]\n",
    "    \n",
    "    R_train = np.full_like(R_np, np.nan)\n",
    "    R_test  = np.full_like(R_np, np.nan)\n",
    "    \n",
    "    R_train[u_idx[train_sel], i_idx[train_sel]] = R_np[u_idx[train_sel], i_idx[train_sel]]\n",
    "    R_test[u_idx[test_sel],   i_idx[test_sel]]   = R_np[u_idx[test_sel],   i_idx[test_sel]]\n",
    "    \n",
    "    return (R_train,R_test)\n",
    "    # TODO: Create a pivot table.\n",
    "    # The index should be 'user_id', columns 'item_id', and values 'rating'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: The Theory of Matrix Factorization\n",
    "\n",
    "The core idea is to **decompose** our large, sparse user-item matrix $R$ (size $m \\times n$) into two smaller, dense matrices:\n",
    "1.  A **user-feature matrix** $P$ (size $m \\times k$).\n",
    "2.  An **item-feature matrix** $Q$ (size $n \\times k$).\n",
    "\n",
    "Here, $k$ is the number of **latent factors**, which is a hyperparameter we choose. These latent factors represent hidden characteristics of users and items. For movies, a factor might represent the \"amount of comedy\" vs. \"drama\", or \"blockbuster\" vs. \"indie film\". For users, a factor might represent their preference for these characteristics.\n",
    "\n",
    "\n",
    "\n",
    "The prediction of a rating $\\hat{r}_{ui}$ that user $u$ would give to item $i$ is calculated by the dot product of the user's latent vector $p_u$ and the item's latent vector $q_i$:\n",
    "\n",
    "$$\\hat{r}_{ui} = p_u \\cdot q_i^T = \\sum_{k=1}^{K} p_{uk} q_{ik}$$\n",
    "\n",
    "Our goal is to find the matrices $P$ and $Q$ such that their product $P \\cdot Q^T$ is as close as possible to the known ratings in our original matrix $R$. We formalize this using a **loss function**. A common choice is the sum of squared errors, with **regularization** to prevent overfitting:\n",
    "\n",
    "$$L = \\sum_{(u,i) \\in \\mathcal{K}} (r_{ui} - \\hat{r}_{ui})^2 + \\lambda \\left( \\sum_{u} ||p_u||^2 + \\sum_{i} ||q_i||^2 \\right)$$\n",
    "\n",
    "Where:\n",
    "* $\\mathcal{K}$ is the set of $(u, i)$ pairs for which the rating $r_{ui}$ is known.\n",
    "* $\\lambda$ is the regularization parameter, another hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: The Algorithm - Stochastic Gradient Descent (SGD)\n",
    "\n",
    "To minimize our loss function $L$, we will use **Stochastic Gradient Descent (SGD)**. Instead of calculating the gradient over all known ratings (which is computationally expensive), SGD iterates through each known rating one by one and updates the parameters in the direction that minimizes the error for that single rating.\n",
    "\n",
    "For each known rating $r_{ui}$:\n",
    "1.  Calculate the prediction error: $e_{ui} = r_{ui} - \\hat{r}_{ui}$\n",
    "2.  Update the user and item latent vectors ($p_u$ and $q_i$) using the following update rules:\n",
    "\n",
    "$$p_u \\leftarrow p_u + \\alpha \\cdot (e_{ui} \\cdot q_i - \\lambda \\cdot p_u)$$\n",
    "$$q_i \\leftarrow q_i + \\alpha \\cdot (e_{ui} \\cdot p_u - \\lambda \\cdot q_i)$$\n",
    "\n",
    "Where:\n",
    "* $\\alpha$ is the **learning rate**, a hyperparameter that controls the step size.\n",
    "\n",
    "We repeat this process for a fixed number of **epochs** (iterations over the entire training dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Step-by-Step Implementation\n",
    "\n",
    "Let's build our model. First, we need to split our data into a training and a testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Initialization\n",
    "\n",
    "We need to initialize our user-feature matrix $P$ and item-feature matrix $Q$ with small random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_matrices(n_users, n_items, n_factors):\n",
    "    \"\"\"\n",
    "    Initializes the user-feature (P) and item-feature (Q) matrices.\n",
    "\n",
    "    Args:\n",
    "        n_users (int): Number of users.\n",
    "        n_items (int): Number of items.\n",
    "        n_factors (int): Number of latent factors.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - P (np.ndarray): The user-feature matrix (n_users x n_factors).\n",
    "            - Q (np.ndarray): The item-feature matrix (n_items x n_factors).\n",
    "    \"\"\"\n",
    "    # TODO: Initialize P and Q with small random values from a standard normal distribution.\n",
    "    P = np.random.normal(scale=0.1, size=(n_users, n_factors))\n",
    "    Q = np.random.normal(scale=0.1, size=(n_items, n_factors))\n",
    "    return (P,Q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 The Training Loop (SGD)\n",
    "\n",
    "This is the core of our algorithm. We will loop for a specified number of epochs. In each epoch, we will iterate over all known ratings in our training set `R_train` and update the corresponding user and item vectors in `P` and `Q`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(R_train, P, Q, learning_rate, regularization, epochs):\n",
    "    R_train = np.nan_to_num(R_train, nan=0.0)\n",
    "    r_predict = np.zeros(R_train.shape)\n",
    "    e = np.zeros(R_train.shape)\n",
    "    n_users, n_items = R_train.shape\n",
    "    for epoch in range(epochs):\n",
    "        L=0\n",
    "        for u in range(n_users):\n",
    "            for i in range(n_items):\n",
    "                if R_train[u][i] > 0:\n",
    "                    r_predict[u][i] = P[u].dot(Q[i])\n",
    "                    e[u][i]= R_train[u][i]-r_predict[u][i]\n",
    "                    p_u_old = P[u].copy()\n",
    "                    P[u]=P[u]+learning_rate*(e[u][i]*Q[i]-regularization*P[u])\n",
    "                    Q[i]=Q[i]+learning_rate*(e[u][i]*p_u_old-regularization*Q[i])\n",
    "                    L=L+(e[u][i]*e[u][i])+regularization*(np.sum(P[u]**2)+np.sum(Q[i]**2))\n",
    "        print (\"Epoch\",epoch,\"loss\",L)\n",
    "    \n",
    "                \n",
    "    return (P,Q)\n",
    "                \n",
    "                \n",
    "\n",
    "            \n",
    "        \n",
    "    \"\"\"\n",
    "    Trains the matrix factorization model using SGD.\n",
    "\n",
    "    Args:\n",
    "        R_train (np.ndarray): The training user-item matrix.\n",
    "        P (np.ndarray): The user-feature matrix.\n",
    "        Q (np.ndarray): The item-feature matrix.\n",
    "        learning_rate (float): The learning rate (alpha).\n",
    "        regularization (float): The regularization parameter (lambda).\n",
    "        epochs (int): The number of iterations over the training data.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the trained P and Q matrices.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Evaluation\n",
    "\n",
    "After training, we must evaluate how well our model performs on unseen data. We will use the **Root Mean Squared Error (RMSE)**, which measures the average magnitude of the errors between predicted and actual ratings.\n",
    "\n",
    "The formula is:\n",
    "$$RMSE = \\sqrt{\\frac{1}{|\\mathcal{T}|} \\sum_{(u,i) \\in \\mathcal{T}} (r_{ui} - \\hat{r}_{ui})^2}$$\n",
    "\n",
    "Where $\\mathcal{T}$ is the set of ratings in our test set. A lower RMSE means better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rmse(R_test, P, Q):\n",
    "    R_test = np.nan_to_num(R_test, nan=0.0)\n",
    "    r_predict = np.zeros(R_test.shape)\n",
    "    error=0\n",
    "    T=0\n",
    "    n_users, n_items = R_test.shape\n",
    "    for u in range(n_users):\n",
    "        for i in range(n_items):\n",
    "            if R_test[u][i] > 0:\n",
    "                r_predict[u][i] = P[u].dot(Q[i])\n",
    "                error= error+ (R_test[u][i]-r_predict[u][i])**2\n",
    "                T=T+1\n",
    "    RMSE=np.sqrt(error/T)\n",
    "    return (RMSE)\n",
    "                \n",
    "    \"\"\"\n",
    "    Calculates the Root Mean Squared Error (RMSE) on the test set.\n",
    "\n",
    "    Args:\n",
    "        R_test (np.ndarray): The testing user-item matrix.\n",
    "        P (np.ndarray): The trained user-feature matrix.\n",
    "        Q (np.ndarray): The trained item-feature matrix.\n",
    "\n",
    "    Returns:\n",
    "        float: The RMSE value.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Putting It All Together\n",
    "\n",
    "Now, let's connect all the pieces. We'll set our hyperparameters, initialize our matrices, train the model, and finally evaluate it.\n",
    "\n",
    "**Your Goal:** Tune the hyperparameters to achieve an **RMSE below 0.98**. A good model can even reach ~0.95. If your RMSE is higher, try adjusting the learning rate, regularization, number of factors, or epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss 687292.6986978222\n",
      "Epoch 1 loss 186748.80424357997\n",
      "Epoch 2 loss 112689.6091035388\n",
      "Epoch 3 loss 100883.27571388651\n",
      "Epoch 4 loss 95945.00862286352\n",
      "Epoch 5 loss 92722.09910743852\n",
      "Epoch 6 loss 90139.86972731406\n",
      "Epoch 7 loss 87865.60185511323\n",
      "Epoch 8 loss 85785.93984287042\n",
      "Epoch 9 loss 83848.94630109498\n",
      "Epoch 10 loss 82021.13550095049\n",
      "Epoch 11 loss 80282.80109909673\n",
      "Epoch 12 loss 78626.74972327506\n",
      "Epoch 13 loss 77053.80921299422\n",
      "Epoch 14 loss 75568.03979182869\n",
      "Epoch 15 loss 74173.5156795197\n",
      "Epoch 16 loss 72872.72321723425\n",
      "Epoch 17 loss 71666.05000244774\n",
      "Epoch 18 loss 70551.87355242026\n",
      "Epoch 19 loss 69526.91977132851\n",
      "Epoch 20 loss 68586.70088252633\n",
      "Epoch 21 loss 67725.93518443446\n",
      "Epoch 22 loss 66938.90593298404\n",
      "Epoch 23 loss 66219.74667994933\n",
      "Epoch 24 loss 65562.65530388737\n",
      "Epoch 25 loss 64962.04536863662\n",
      "Epoch 26 loss 64412.64550202787\n",
      "Epoch 27 loss 63909.55753453382\n",
      "Epoch 28 loss 63448.28328773344\n",
      "Epoch 29 loss 63024.728633461935\n",
      "Epoch 30 loss 62635.191987418104\n",
      "Epoch 31 loss 62276.34291382115\n",
      "Epoch 32 loss 61945.19512672265\n",
      "Epoch 33 loss 61639.07696687221\n",
      "Epoch 34 loss 61355.60145285967\n",
      "Epoch 35 loss 61092.637253976915\n",
      "Epoch 36 loss 60848.28138506118\n",
      "Epoch 37 loss 60620.834042858405\n",
      "Epoch 38 loss 60408.775749836124\n",
      "Epoch 39 loss 60210.74680973067\n",
      "Epoch 40 loss 60025.52898124191\n",
      "Epoch 41 loss 59852.029221465025\n",
      "Epoch 42 loss 59689.26532372804\n",
      "Epoch 43 loss 59536.353265555656\n",
      "Epoch 44 loss 59392.49608460229\n",
      "Epoch 45 loss 59256.97410918581\n",
      "Epoch 46 loss 59129.13638252311\n",
      "Epoch 47 loss 59008.39313417481\n",
      "Epoch 48 loss 58894.20916699801\n",
      "Epoch 49 loss 58786.09804267965\n",
      "0.9753564529650682\n"
     ]
    }
   ],
   "source": [
    "# --- Hyperparameters ---\n",
    "# Number of latent factors (k)\n",
    "# Learning rate (alpha)\n",
    "# Regularization parameter (lambda)\n",
    "# Number of epochs\n",
    "k=50\n",
    "learning_rate=0.01\n",
    "regularization=0.04\n",
    "epochs=50\n",
    "\n",
    "\n",
    "# --- Initialization ---\n",
    "# Remember user and item IDs are 1-based, but our numpy arrays are 0-based.\n",
    "# The number of users/items from the shape of R_df is correct for 0-based indexing.\n",
    "\n",
    "R_train,R_test=create_user_item_matrix(df)\n",
    "n_users,n_items= R_train.shape\n",
    "P,Q= initialize_matrices(n_users, n_items, k)\n",
    "\n",
    "\n",
    "\n",
    "# --- Training ---\n",
    "P_train,Q_train=train_model(R_train, P, Q, learning_rate, regularization, epochs)\n",
    "\n",
    "\n",
    "# --- Evaluation ---\n",
    "RMSE=calculate_rmse(R_test, P_train,Q_train)\n",
    "print(RMSE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Making Recommendations\n",
    "\n",
    "The ultimate goal is to recommend movies! Now that we have our trained matrices $P$ and $Q$, we can predict the rating for *any* user-item pair, including those the user has not seen yet.\n",
    "\n",
    "The process for a given user `user_id`:\n",
    "1.  Get the user's latent vector $p_u$ from the trained matrix $P$.\n",
    "2.  Calculate the predicted ratings for all items by taking the dot product of $p_u$ and the entire item-feature matrix $Q^T$.\n",
    "3.  Create a list of movie titles and their predicted ratings.\n",
    "4.  Filter out movies the user has already seen.\n",
    "5.  Sort the remaining movies by their predicted rating in descending order.\n",
    "6.  Return the top N movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_top_movies(user_id, P, Q, movie_titles_df, R_df, top_n=10):\n",
    "    \"\"\"\n",
    "    Recommends top N movies for a given user.\n",
    "\n",
    "    Args:\n",
    "        user_id (int): The ID of the user.\n",
    "        P (np.ndarray): The trained user-feature matrix.\n",
    "        Q (np.ndarray): The trained item-feature matrix.\n",
    "        movie_titles_df (pd.DataFrame): Dataframe with item_id and title.\n",
    "        R_df (pd.DataFrame): The original user-item matrix dataframe (for checking seen movies).\n",
    "        top_n (int): The number of movies to recommend.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe with the top N recommended movie titles and their predicted ratings.\n",
    "    \"\"\"\n",
    "    movie_titles = movie_titles_df['title'].values\n",
    "    R = R_df.values\n",
    "\n",
    "    num_items = R.shape[1]\n",
    "    r_predict = np.full(num_items, -np.inf) \n",
    "\n",
    "    for i in range(num_items):\n",
    "        if np.isnan(R[user_id, i]):\n",
    "            r_predict[i] = P[user_id].dot(Q[i])\n",
    "\n",
    "    top_indices = np.argsort(r_predict)[::-1][:top_n]\n",
    "\n",
    "    recommendations = np.vstack((movie_titles[top_indices],  np.round(r_predict[top_indices], 2))).T\n",
    "\n",
    "    return recommendations\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_R_df(df):\n",
    "    df=df[[\"user_id\",\"rating\",\"title\"]]\n",
    "    R_df = df.pivot_table(index=\"user_id\", columns=\"title\", values=\"rating\")\n",
    "    \n",
    "    return (R_df)\n",
    "\n",
    "R_df=create_R_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations= recommend_top_movies(1, P_train, Q_train, df, R_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Rock, The (1996)' 4.85]\n",
      " ['Adventures of Priscilla, Queen of the Desert, The (1994)' 4.84]\n",
      " ['Last of the Mohicans, The (1992)' 4.81]\n",
      " ['Cliffhanger (1993)' 4.81]\n",
      " ['Rock, The (1996)' 4.75]\n",
      " [\"Marvin's Room (1996)\" 4.73]\n",
      " ['Mirror Has Two Faces, The (1996)' 4.71]\n",
      " ['Taxi Driver (1976)' 4.7]\n",
      " ['Jaws (1975)' 4.68]\n",
      " ['Terminator, The (1984)' 4.68]]\n"
     ]
    }
   ],
   "source": [
    "print (recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
